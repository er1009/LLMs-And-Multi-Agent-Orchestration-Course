{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 Analysis: Needle in Haystack (Lost in the Middle)\n",
    "\n",
    "This notebook analyzes results from Experiment 1, which tests the \"Lost in the Middle\" phenomenon where LLMs struggle to retrieve information from the middle of long contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw results\n",
    "results_path = Path(\"../results/experiment1/raw_results.json\")\n",
    "\n",
    "with open(results_path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Create DataFrame from trials\n",
    "df = pd.DataFrame(results['trials'])\n",
    "\n",
    "print(f\"Total trials: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "print(\"=== Overall Statistics ===\")\n",
    "print(f\"Mean Accuracy: {results['statistics']['mean_accuracy']:.2%}\")\n",
    "print(f\"Std Accuracy: {results['statistics']['std_accuracy']:.3f}\")\n",
    "print(f\"Mean Latency: {results['statistics']['mean_latency']:.0f}ms\")\n",
    "print(f\"Confidence Interval (95%): {results['statistics']['confidence_interval_95']}\")\n",
    "\n",
    "# By position\n",
    "print(\"\\n=== Accuracy by Position ===\")\n",
    "position_stats = df.groupby('position')['accuracy'].agg(['mean', 'std', 'count'])\n",
    "position_stats['mean'] = position_stats['mean'] * 100  # Convert to percentage\n",
    "print(position_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot by position\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy by position\n",
    "df['accuracy_pct'] = df['accuracy'] * 100\n",
    "sns.boxplot(data=df, x='position', y='accuracy_pct', ax=axes[0])\n",
    "axes[0].set_title('Accuracy Distribution by Position', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_xlabel('Fact Position', fontsize=12)\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "# Latency by position\n",
    "sns.boxplot(data=df[df['error'].isna()], x='position', y='latency_ms', ax=axes[1])\n",
    "axes[1].set_title('Latency Distribution by Position', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Latency (ms)', fontsize=12)\n",
    "axes[1].set_xlabel('Fact Position', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Get accuracy for each position\n",
    "start_acc = df[df['position'] == 'start']['accuracy'].values\n",
    "middle_acc = df[df['position'] == 'middle']['accuracy'].values\n",
    "end_acc = df[df['position'] == 'end']['accuracy'].values\n",
    "\n",
    "# T-tests\n",
    "print(\"=== Statistical Significance Tests ===\")\n",
    "print(\"\\nStart vs Middle:\")\n",
    "t_stat, p_value = stats.ttest_ind(start_acc, middle_acc)\n",
    "print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant at p<0.05: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "print(\"\\nMiddle vs End:\")\n",
    "t_stat, p_value = stats.ttest_ind(middle_acc, end_acc)\n",
    "print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant at p<0.05: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors\n",
    "errors = df[df['error'].notna()]\n",
    "print(f\"Total errors: {len(errors)}\")\n",
    "\n",
    "if len(errors) > 0:\n",
    "    print(\"\\nError details:\")\n",
    "    print(errors[['trial_id', 'position', 'error']])\n",
    "else:\n",
    "    print(\"No errors occurred during the experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "Based on the analysis:\n",
    "\n",
    "1. **Lost in the Middle Effect**: [To be filled based on results]\n",
    "2. **Statistical Significance**: [To be filled based on results]\n",
    "3. **Performance Implications**: [To be filled based on results]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
